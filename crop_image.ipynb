{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import time, timeit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default colors\n",
    "COLOR_BLUE = (255, 0, 0)\n",
    "COLOR_GREEN = (0, 255, 0)\n",
    "COLOR_RED = (0, 0, 255)\n",
    "COLOR_WHITE = (255, 255, 255)\n",
    "COLOR_YELLOW = (0, 255, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_WIDTH, IMG_HEIGHT = 416, 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = '/Users/dongth/Documents/github/wp_face_detection/yolo/yolov3-face.cfg'\n",
    "WEIGHT = '/Users/dongth/Documents/github/wp_face_detection/yolo/yolov3-wider_16000.weights'\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(MODEL, WEIGHT)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model():\n",
    "    '''\n",
    "    Load the pre-trained YOLO model\n",
    "    '''\n",
    "    MODEL = '/Users/dongth/Documents/github/wp_face_detection/yolo/yolov3-face.cfg'\n",
    "    WEIGHT = '/Users/dongth/Documents/github/wp_face_detection/yolo/yolov3-wider_16000.weights'\n",
    "\n",
    "    net = cv2.dnn.readNetFromDarknet(MODEL, WEIGHT)\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_forward(frame):\n",
    "    '''\n",
    "    Pass each captured frame into the pretrained yolo model\n",
    "    '''\n",
    "    # Makeing blob object from the original image\n",
    "    blob = cv2.dnn.blobFromImage(frame,\n",
    "                                1/255, (IMG_WIDTH, IMG_HEIGHT),\n",
    "                                [0,0,0], 1, crop=False)\n",
    "\n",
    "    # Set model input\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Define the layers that we want to get the outputs from\n",
    "    output_layers = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "    # Run prediction\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bounding_boxes(frame, outs, conf_thresh, nms_thresh):\n",
    "    '''\n",
    "    Scan through all the bounding boxes output from the network and keep only\n",
    "    the ones with high confidence scores. Assign the box's class label as the\n",
    "    class with the highest score.'''\n",
    "    frame_height = frame.shape[0]\n",
    "    frame_width = frame.shape[1]\n",
    "\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    final_boxes = []\n",
    "\n",
    "    # Each frame produces 3 outs correspoding to 3 output layers\n",
    "    for out in outs:\n",
    "        # One out has multiple predictions for multiple captured objects.\n",
    "        for detection in out:\n",
    "            confidence = detection[-1]\n",
    "            # Extract position data of face area (only area with high confidence)\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * frame_width)\n",
    "                center_y = int(detection[1] * frame_height)\n",
    "                width = int(detection[2] * frame_width)\n",
    "                height = int(detection[3] * frame_height)\n",
    "\n",
    "                # Find the top left point of the bounding box\n",
    "                topleft_x = int(center_x - width//2)\n",
    "                topleft_y = int(center_y - height//2)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([topleft_x, topleft_y, width, height])\n",
    "\n",
    "    # Perform non-maximum suppression to eliminate \n",
    "    # redundant overlapping boxes with lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_thresh, nms_thresh)\n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        confidence = confidences[i]\n",
    "        final_boxes.append((box, confidence))\n",
    "\n",
    "    return final_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bounding_boxes(frame, boxes, dir):\n",
    "    \n",
    "    for box, confidence in boxes:\n",
    "        # Extract position data\n",
    "        topleft_x = box[0]\n",
    "        topleft_y = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "\n",
    "        bottomright_x = topleft_x + width\n",
    "        bottomright_y = topleft_y + height\n",
    "\n",
    "        # Crop frame\n",
    "        crop_img = frame[(topleft_y-15):(topleft_y+height+15),(topleft_x-10):(topleft_x+width+10)]\n",
    "        now = datetime.now().strftime(\"%H%M%S\")\n",
    "        img_name = os.path.join(dir,f'{now}.jpg')\n",
    "        cv2.imwrite(img_name, crop_img) # Save cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Users/dongth/Documents/github/wp_face_detection/model_train/DATASET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(DATA_DIR)\n",
    "image_count = len(list(data_dir.glob('*/*')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directory path for train folder of each class\n",
    "\n",
    "dong_dir = '/Users/dongth/Documents/github/wp_face_detection/model_train/DATASET/Dong'\n",
    "vy_dir = '/Users/dongth/Documents/github/wp_face_detection/model_train/DATASET/Vy'\n",
    "thao_dir = '/Users/dongth/Documents/github/wp_face_detection/model_train/DATASET/Thao'\n",
    "lili_dir = '/Users/dongth/Documents/github/wp_face_detection/model_train/DATASET/Lili'\n",
    "hiep_dir = '/Users/dongth/Documents/github/wp_face_detection/model_train/DATASET/Hiep'\n",
    "all_dir = [dong_dir, vy_dir, thao_dir, lili_dir, hiep_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directory list of images from train folder of each class\n",
    "\n",
    "dong_fnames = os.listdir(dong_dir)\n",
    "vy_fnames = os.listdir(vy_dir)\n",
    "thao_fnames = os.listdir(thao_dir)\n",
    "lili_fnames = os.listdir(lili_dir)\n",
    "hiep_fnames = os.listdir(hiep_dir)\n",
    "all_fnames = [dong_fnames,vy_fnames,thao_fnames,lili_fnames,hiep_fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['straight_121.png',\n",
       " 'straight_109.png',\n",
       " 'free_103.png',\n",
       " 'free_102.png',\n",
       " 'free_116.png']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dong_fnames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 50\n",
      "Successfully saved 100\n",
      "Successfully saved 50\n",
      "Successfully saved 100\n",
      "Successfully saved 50\n",
      "Successfully saved 100\n",
      "Successfully saved 50\n",
      "Successfully saved 100\n",
      "Successfully saved 50\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/Users/dongth/Documents/github/wp_face_detection/model_train/DATASET_CROP\"\n",
    "# Constants\n",
    "conf_thresh = 0.5\n",
    "nms_thresh = 0.4\n",
    "\n",
    "# LOOP over each image in each directory\n",
    "for i, fnames in enumerate(all_fnames):\n",
    "    for c, fname in enumerate(fnames):\n",
    "        img_path = os.path.join(all_dir[i], fname)\n",
    "        label = img_path.split(\"/\")[-2]\n",
    "        dir = os.path.join(DATA_DIR, label)\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # Find the bounding box\n",
    "        net = load_yolo_model() # Load the yolo model\n",
    "        outs = yolo_forward(img) # Pass the image to the model\n",
    "        final_boxes = find_bounding_boxes(img, outs, conf_thresh, nms_thresh) # Bounding boxes after NMS\n",
    "        crop_bounding_boxes(img, final_boxes, dir) # Crop and save bounding box(es) image with labels\n",
    "        if (c+1) % 50 == 0:\n",
    "            print(f\"Successfully saved {c+1}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "646b4d9e3f3681b5891a540668e3535ee1a880d9150ef32fd737c6a781f9d79c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('new_venv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
